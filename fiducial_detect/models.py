# AUTOGENERATED! DO NOT EDIT! File to edit: models.ipynb (unless otherwise specified).

__all__ = ['get_out_channels', 'conv_layer', 'up_conv_layer', 'get_encoder', 'UNET']

# Cell
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from IPython.core.debugger import set_trace
from torch.utils.data import DataLoader

from .data import *

# Cell
def get_out_channels(m):
    # Gets out_channels of last conv layer
    out_channels = None
    for m_sub in m.modules():
        if isinstance(m_sub, nn.modules.conv._ConvNd):
            out_channels = m_sub.out_channels
    return out_channels

# Cell
def conv_layer(in_channels, out_channels, kernel_size, stride, padding):
    return nn.Sequential(
        nn.Conv2d(in_channels,
                  out_channels,
                  kernel_size=kernel_size,
                  stride=stride,
                  padding=padding,
                  bias=False), # No bias needed since learnable affine params in norm layer
        nn.GroupNorm(1, out_channels),
        nn.ReLU(inplace=True)
    )

# Cell
def up_conv_layer(in_channels, out_channels, kernel_size, stride, padding, scale_factor):
    return nn.Sequential(
        conv_layer(in_channels, out_channels, kernel_size, stride, padding),
        nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=True)
    )

# Cell
def get_encoder(in_channels, layout_encoder):
    prev_out_channels = in_channels
    encoder = []
    for layout_layer in layout_encoder:
        layer = []
        for layout_conv in layout_layer:
            layer.append(conv_layer(prev_out_channels, *layout_conv))
            prev_out_channels = layout_conv[0]
        encoder.append(nn.Sequential(*layer))
    return nn.Sequential(*encoder)

# Cell
class UNET(nn.Module):
    def __init__(self, encoder, out_channels):
        super(UNET, self).__init__()
        self.encoder = encoder
        self.decoder = self._get_decoder(encoder)
        self.last_conv = nn.Conv2d(get_out_channels(self.decoder),
                                   out_channels,
                                   kernel_size=3,
                                   stride=1,
                                   padding=1)

    def _get_decoder(self, encoder):
        # Assumes each level of encoder shrinks by a factor of 2
        decoder = []
        for i in reversed(range(len(encoder))):
            in_channels = get_out_channels(encoder[i])
            if len(decoder) > 0:
                in_channels += get_out_channels(decoder[-1])
            decoder.append(up_conv_layer(in_channels,
                                         get_out_channels(encoder[i])//2,
                                         kernel_size=3,
                                         stride=1,
                                         padding=1,
                                         scale_factor=2))
        return nn.Sequential(*decoder)

    def forward(self, X):
        Xs = [X]
        for conv_down in self.encoder:
            Xs.append(conv_down(Xs[-1]))
        X = Xs[-1][:,0:0,:,:] # Empty, but same size and dimension as last X
        for idx, conv_up in enumerate(self.decoder):
            X = conv_up(torch.cat([X, Xs[-(idx+1)]], dim=1))
        return self.last_conv(X)